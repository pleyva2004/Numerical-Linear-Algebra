{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Eigendecomposition\n",
        "---\n",
        "Eigendecomposition is considered the heart of linear algebra.\n",
        "Eigendecomposition has a geometric interpretation (axis of rotation invariance)\n",
        "a statistical interpretation (axis of maximal covariance), a dynamical-systems interpretation (stable system states), a graph-theoretic\n",
        "interpretation (the impact of a node on its network), a financial-market interpretation (identifying\n",
        "stocks that covary), and many more.\n",
        "Eigendecomposition (and the SVD, which we will cover later\n",
        "and is closely related to eigendecomposition) is among the most important contributions of\n",
        "algebra to data science.\n",
        "\n",
        "**Geometry**: There is a special combination of matrix and a vector such that the matrix stretched but did not rotate vector. That vector is an eigenvector of the matrix, and the amount of stretching is eigenvalue.\n",
        "\n",
        "**Statistics** (Principal Components Analysis):\n",
        "We apply statistics to identity and quantity relationships\n",
        "between variables. For example, the rise of global temperatures correlates with the decline in the number of birds, but how strong is that relationship? There are tens of thousands of cryptocoins and we can ask whether the entirety of\n",
        "the cryptospace operates as a single system (meaning that the value of all coins gcup and down together), or whether there are independent subcategories within that space (meaning that some coins or groups of coins change in value independently of the value or other coins). Such hypothesis can be tested by\n",
        "performing a principal components analysis on dataset that contains the prices of various cryptocoins over time.\n",
        "\n",
        "**Noise Reduction**: Most datasets contain noise. One method of reducing random noise is to identity the eigenvalues and eigenvectors\n",
        "of a system, and \"project out\" directions in the data space associated with small eigenvalues. The assumption is that random noise makes a relativelv small contribution to the total variance. \"Projecting out\" a data dimension means to reconstruct a dataset after setting some eigenvalues that are below some threshold to zero.\n",
        "\n",
        "**Dimension Reduction** (Data Compression):\n",
        "One way to dimension-reduce a dataset is to take its eigendecomposition, drop\n",
        "the eigenvalues and eigenvectors associated with small directions in the data space and then transmit only the relatively larger eigenvector/value pairs.  The main idea is to decompose the dataset into\n",
        "a set of basis vectors that capture the most important teatures of the data, and then reconstruct a high-quality version of the original data.\n"
      ],
      "metadata": {
        "id": "u4T3nKWNtji3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eigenvalues and Eigenvectors\n",
        "----------------------------\n",
        "\n",
        "An **_eigenvalue_** of an $n \\times n$ matrix $\\mathbf{A}$ is a scalar $\\lambda$ such that $\\mathbf{A} {\\bf x} = \\lambda {\\bf x}$ for some non-zero vector ${\\bf x}$. The eigenvalue $\\lambda$ can be any real or complex scalar, $\\lambda \\in \\mathbb{R}\\ \\text{or } \\lambda \\in \\mathbb{C}$. Eigenvalues can be complex even if all the entries of the matrix $\\mathbf{A}$ are real. In this case, the corresponding vector ${\\bf x}$ must have complex-valued components, ${\\bf x}\\in \\mathbb{C}^n$. The equation $\\mathbf{A}\\mathbf{x}=\\lambda\\mathbf{x}$ is called the **_eigenvalue equation_** and any such non-zero vector ${\\bf x}$ is called an **_eigenvector_** of $\\mathbf{A}$ corresponding to $\\lambda$.\n",
        "\n",
        "The eigenvalue equation can be rearranged to $(\\mathbf{A} - \\lambda {\\bf I}) {\\bf x} = 0$. Since the eigenvector should be a nonzero vector, therfore:\n",
        "\n",
        "1. The column or rows of $(A-\\lambda I)$ are linearly dependent\n",
        "2. $(A-\\lambda I)$ is not full rank, $Rank(A)<n$.\n",
        "3. $(A-\\lambda I)$ is not invertible.\n",
        "4. $\\operatorname{det}(A-\\lambda I)=0$, which is called **_characteristic equation_**.\n",
        "\n",
        "The expression $p(\\lambda) = \\operatorname{det}(\\mathbf{A} - \\lambda {\\bf I})$ is called the **_characteristic polynomial_** and is a polynomial of degree $n$.\n",
        "Eigenvalues can be found by solving the characteristic equation, however, this is not a good numerical approach for finding eigenvalues.\n",
        "\n",
        "Consider writing eigenvalues ordered by magnitude:\n",
        "\n",
        "$$\n",
        "|\\lambda_1| \\geq |\\lambda_2| \\geq \\cdots \\geq |\\lambda_n|,\n",
        "$$\n",
        "\n",
        "and normalizing eigenvectors, so that $\\|{\\bf x}\\| = 1$."
      ],
      "metadata": {
        "id": "6Jku8eW8ZzMB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzs4FWjFA_2P"
      },
      "source": [
        "Consider a matrix $A$\n",
        "\n",
        "$$\n",
        "A = \\begin{bmatrix}\n",
        "1 & 0 & 0 \\\\\n",
        "1 & 0 & 1 \\\\\n",
        "2 & -2 & 3\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Set up the characteristic equation,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3D0aO2wA_2P"
      },
      "source": [
        "$$\n",
        "\\text{det}\\left(\n",
        "\\begin{bmatrix}\n",
        "1 & 0 & 0 \\\\\n",
        "1 & 0 & 1 \\\\\n",
        "2 & -2 & 3\n",
        "\\end{bmatrix}\n",
        "-\n",
        "\\lambda\n",
        "\\begin{bmatrix}\n",
        "1 & 0 & 0 \\\\\n",
        "0 & 1 & 0 \\\\\n",
        "0 & 0 & 1\n",
        "\\end{bmatrix}\n",
        "\\right) = 0\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twgeTKuMA_2Q"
      },
      "source": [
        "Use SymPy ```charpoly``` and ```factor```, we can have straightforward solutions for eigenvalues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwwCD9BYA_2L"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import numpy as np\n",
        "import sympy as sy\n",
        "sy.init_printing()\n",
        "plt.style.use('ggplot')\n",
        "np.set_printoptions(precision=3)\n",
        "np.set_printoptions(suppress=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIylLbuzA_2Q"
      },
      "outputs": [],
      "source": [
        "lamda = sy.symbols('lamda') # 'lamda' withtout 'b' is reserved for SymPy, lambda is reserved for Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9S6tsdPjA_2R"
      },
      "source": [
        "```charpoly``` returns characteristic equation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqGrY6HMA_2S"
      },
      "outputs": [],
      "source": [
        "A = sy.Matrix([[1, 0, 0], [1, 0, 1], [2, -2, 3]])\n",
        "p = A.charpoly(lamda); p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5Ia2wX1A_2T"
      },
      "source": [
        "Factor the polynomial such that we can see the solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L83L0GnoA_2T"
      },
      "outputs": [],
      "source": [
        "factored_poly = sy.factor(p)\n",
        "print(factored_poly)\n",
        "x = sy.symbols('lamda')\n",
        "expression = factored_poly.as_expr()\n",
        "factored_expression = sy.factor(expression)\n",
        "print(factored_expression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjQR3i5GA_2U"
      },
      "source": [
        "From the factored characteristic polynomial, we get the eigenvalue, and $\\lambda =1$ has algebraic multiplicity of $2$, because there are two $(\\lambda-1)$. If not factored, we can use ```solve``` instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAs8oCjjA_2U"
      },
      "outputs": [],
      "source": [
        "sy.solve(p,lamda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2I9lYPpqA_2U"
      },
      "source": [
        "Or use ```eigenvals``` directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLFOwUk8A_2U"
      },
      "outputs": [],
      "source": [
        "A.eigenvals()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6NfnHp-A_2V"
      },
      "source": [
        "To find the eigenvector corresponding to $\\lambda$, we substitute the eigenvalues back into $(A-\\lambda I)x=0$ and solve it. Construct augmented matrix with $\\lambda =1$ and perform rref."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FxYOo0DA_2V"
      },
      "outputs": [],
      "source": [
        "(A - 1*sy.eye(3)).row_join(sy.zeros(3,1)).rref()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xrc8pn3zA_2V"
      },
      "source": [
        "The null space is the solution set of the linear system.\n",
        "\n",
        "$$\n",
        "\\left[\n",
        "\\begin{matrix}\n",
        "x_1 \\\\ x_2 \\\\ x_3\n",
        "\\end{matrix}\n",
        "\\right]=\n",
        "\\left[\n",
        "\\begin{matrix}\n",
        "x_2-x_3 \\\\ x_2 \\\\ x_3\n",
        "\\end{matrix}\n",
        "\\right]=\n",
        "x_2\\left[\n",
        "\\begin{matrix}\n",
        "1 \\\\ 1 \\\\ 0\n",
        "\\end{matrix}\n",
        "\\right]\n",
        "+x_3\\left[\n",
        "\\begin{matrix}\n",
        "-1 \\\\ 0 \\\\ 1\n",
        "\\end{matrix}\n",
        "\\right]\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pfMh8RFA_2V"
      },
      "source": [
        "This is called _eigenspace_ for $\\lambda = 1$, which is a subspace in $\\mathbb{R}^3$. All eigenvectors are inside the eigenspace.\n",
        "\n",
        "We can proceed with $\\lambda = 2$ as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ir6IfBxZA_2W"
      },
      "outputs": [],
      "source": [
        "(A - 2*sy.eye(3)).row_join(sy.zeros(3,1)).rref()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23jFjgdIA_2W"
      },
      "source": [
        "The null space is the solution set of the linear system.\n",
        "\n",
        "$$\n",
        "\\left[\n",
        "\\begin{matrix}\n",
        "x_1 \\\\ x_2 \\\\ x_3\n",
        "\\end{matrix}\n",
        "\\right]=\n",
        "\\left[\n",
        "\\begin{matrix}\n",
        "0\\\\ \\frac{1}{2}x_3\\\\ x_3\n",
        "\\end{matrix}\n",
        "\\right]=\n",
        "x_3\\left[\n",
        "\\begin{matrix}\n",
        "0 \\\\ \\frac{1}{2} \\\\ 1\n",
        "\\end{matrix}\n",
        "\\right]\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-4_d43NA_2X"
      },
      "source": [
        "To avoid troubles of solving back and forth, SymPy has ```eigenvects``` to calcuate eigenvalues and eigenspaces (basis of eigenspace)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCIVfP_NA_2X"
      },
      "outputs": [],
      "source": [
        "eig = A.eigenvects(); eig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBcnJqvIA_2X"
      },
      "source": [
        "To clarify what we just get, write"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-E-LRRYA_2Y"
      },
      "outputs": [],
      "source": [
        "print('Eigenvalue = {0}, Multiplicity = {1}, Eigenspace = {2}'.format(eig[0][0], eig[0][1], eig[0][2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zu0adwC_A_2Y"
      },
      "outputs": [],
      "source": [
        "print('Eigenvalue = {0}, Multiplicity = {1}, Eigenspace = {2}'.format(eig[1][0], eig[1][1], eig[1][2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJfG-epPA_2Y"
      },
      "source": [
        "## <font face=\"gotham\" color=\"black\"> NumPy Functions for Eigenvalues and Eigenspace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5Ql_B09A_2Z"
      },
      "source": [
        "```.eigvals()``` and ```.eig(A)``` are NumPy functions for eigenvalues and eigenvectors."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matrix = np.array([\n",
        "             [1,2],\n",
        "             [3,4]\n",
        "             ])\n",
        "\n",
        "# get the eigenvalues\n",
        "evals = np.linalg.eig(matrix)[0]\n",
        "evals"
      ],
      "metadata": {
        "id": "wlA3lzFaYVS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding eigenvectors\n",
        "\n",
        "evals,evecs = np.linalg.eig(matrix)\n",
        "print(evals), print(' ')\n",
        "print(evecs)"
      ],
      "metadata": {
        "id": "7VxNv7eEZx-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# same matrix as above\n",
        "evals,evecs = np.linalg.eig(matrix)\n",
        "\n",
        "print('List of eigenvalues:')\n",
        "print(evals)\n",
        "\n",
        "print(f'\\nMatrix of eigenvectors (in the columns!):')\n",
        "print(evecs)"
      ],
      "metadata": {
        "id": "3zygvzknH9og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Geometry of eigenvectors"
      ],
      "metadata": {
        "id": "LJ7QkMH5xVu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import matplotlib_inline.backend_inline\n",
        "matplotlib_inline.backend_inline.set_matplotlib_formats('svg') # display figures in vector format\n",
        "plt.rcParams.update({'font.size':14}) # set global font size\n",
        "\n",
        "# in 2D of course, for visualization\n",
        "\n",
        "# the matrix\n",
        "M = np.array([ [-1,1],\n",
        "               [-1,2] ])\n",
        "\n",
        "# its eigenvalues and eigenvectors\n",
        "eigenvalues,eigenvectors = np.linalg.eig(M)\n",
        "print(eigenvalues)\n",
        "\n",
        "# some random vectors\n",
        "notEigenvectors = np.random.randn(2,2)\n",
        "\n",
        "# multipy to create new vectors\n",
        "Mv = M @ eigenvectors\n",
        "Mw = M @ notEigenvectors\n",
        "\n",
        "\n",
        "\n",
        "## and now plot\n",
        "_,axs = plt.subplots(1,2,figsize=(10,6))\n",
        "\n",
        "# the two eigenvectors\n",
        "axs[0].plot([0,eigenvectors[0,0]],[0,eigenvectors[1,0]],'k',linewidth=2,label='$v_1$')\n",
        "axs[0].plot([0,Mv[0,0]],[0,Mv[1,0]],'k--',linewidth=2,label='$Mv_1$')\n",
        "\n",
        "axs[0].plot([0,eigenvectors[0,1]],[0,eigenvectors[1,1]],'r',linewidth=2,label='$v_2$')\n",
        "axs[0].plot([0,Mv[0,1]],[0,Mv[1,1]],'r--',linewidth=2,label='$Mv_2$')\n",
        "\n",
        "# the two non-eigenvectors\n",
        "axs[1].plot([0,notEigenvectors[0,0]],[0,notEigenvectors[1,0]],'k',linewidth=2,label='$w_1$')\n",
        "axs[1].plot([0,Mw[0,0]],[0,Mw[1,0]],'k--',linewidth=2,label='$Mw_1$')\n",
        "\n",
        "axs[1].plot([0,notEigenvectors[0,1]],[0,notEigenvectors[1,1]],'r',linewidth=2,label='$w_2$')\n",
        "axs[1].plot([0,Mw[0,1]],[0,Mw[1,1]],'r--',linewidth=2,label='$Mw_2$')\n",
        "\n",
        "\n",
        "# adjust the graphs a bit\n",
        "for i in range(2):\n",
        "  axs[i].axis('square')\n",
        "  axs[i].set_xlim([-1.5,1.5])\n",
        "  axs[i].set_ylim([-1.5,1.5])\n",
        "  axs[i].grid()\n",
        "  axs[i].legend()\n",
        "\n",
        "#plt.savefig('Figure_13_01.png',dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mtyEVYsTxY3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing linear transformations\n",
        "\n",
        "We can see the effect of eigenvectors and eigenvalues in linear transformation. We will see first how linear transformation works. Linear transformation is a mapping between an input vector and an output vector. Different operations like projection or rotation are linear transformations.\n",
        "\n",
        "-------\n",
        "\n",
        " Every linear transformations can be though as applying a matrix on the input vector. We will see the meaning of this graphically. For that purpose, let's start by drawing the set of unit vectors (they are all vectors with a norm of 1)."
      ],
      "metadata": {
        "id": "cTmtvix_LnIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linspace(-1, 1, 20)\n",
        "y_u = np.sqrt(1 - x**2)\n",
        "y_d = -np.sqrt(1 - x**2)\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (5, 5))\n",
        "ax.plot(x, y_u, color = 'b')\n",
        "ax.plot(x, y_d, color = 'b')\n",
        "\n",
        "ax.scatter(0, 0, s = 100, fc = 'k', ec = 'r')\n",
        "\n",
        "for i in range(len(x)):\n",
        "    ax.arrow(0, 0, x[i], y_u[i], head_width = .04,\n",
        "             head_length= .17, length_includes_head = True,\n",
        "             width = .001, ec = 'r', fc = 'None')\n",
        "    ax.arrow(0, 0, x[i], y_d[i], head_width = .04,\n",
        "             head_length= .17, length_includes_head = True,\n",
        "             width = .001, ec = 'r', fc = 'None')"
      ],
      "metadata": {
        "id": "pTKk9CQuBGdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we will transform each of these points by applying a matrix\n",
        ". This is the goal of the function bellow that takes a matrix as input and will draw\n",
        "\n",
        "1. the origin set of unit vectors\n",
        "2. the transformed set of unit vectors\n",
        "3. the eigenvectors\n",
        "4. the eigenvectors scalled by their eigenvalues"
      ],
      "metadata": {
        "id": "FKw1D6NuLwRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([[3, -2], [1, 0]])\n",
        "\n",
        "Vu = np.hstack((x[:, np.newaxis], y_u[:, np.newaxis]))\n",
        "AV_u = (A@Vu.T)\n",
        "\n",
        "Vd = np.hstack((x[:, np.newaxis], y_d[:, np.newaxis]))\n",
        "AV_d = (A@Vd.T)\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (8, 8))\n",
        "\n",
        "for i in range(len(x)):\n",
        "    ax.arrow(0, 0, AV_u[0, i], AV_u[1, i], head_width = .18,\n",
        "             head_length= .27, length_includes_head = True,\n",
        "             width = .03, ec = 'darkorange', fc = 'None')\n",
        "    ax.arrow(0, 0, AV_d[0, i], AV_d[1, i], head_width = .18,\n",
        "             head_length= .27, length_includes_head = True,\n",
        "             width = .03, ec = 'darkorange', fc = 'None')\n",
        "ax.axis([-15, 15, -5, 5])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YhzHj1ejT9n2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0b3n01jA_2h"
      },
      "source": [
        "We can plot the cirle and ellipse together, those vectors pointing the same direction before and after the linear transformation are eigenvector of $A$, eigenvalue is the length ratio between them."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plotVectors(vecs, cols, alpha=1):\n",
        "    \"\"\"\n",
        "    Plot set of vectors.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    vecs : array-like\n",
        "        Coordinates of the vectors to plot. Each vectors is in an array. For\n",
        "        instance: [[1, 3], [2, 2]] can be used to plot 2 vectors.\n",
        "    cols : array-like\n",
        "        Colors of the vectors. For instance: ['red', 'blue'] will display the\n",
        "        first vector in red and the second in blue.\n",
        "    alpha : float\n",
        "        Opacity of vectors\n",
        "\n",
        "    Returns:\n",
        "\n",
        "    fig : instance of matplotlib.figure.Figure\n",
        "        The figure of the vectors\n",
        "    \"\"\"\n",
        "    plt.axvline(x=0, color='#A9A9A9', zorder=0)\n",
        "    plt.axhline(y=0, color='#A9A9A9', zorder=0)\n",
        "\n",
        "    for i in range(len(vecs)):\n",
        "        if (isinstance(alpha, list)):\n",
        "            alpha_i = alpha[i]\n",
        "        else:\n",
        "            alpha_i = alpha\n",
        "        x = np.concatenate([[0,0],vecs[i]])\n",
        "        plt.quiver([x[0]],\n",
        "                   [x[1]],\n",
        "                   [x[2]],\n",
        "                   [x[3]],\n",
        "                   angles='xy', scale_units='xy', scale=1, color=cols[i],\n",
        "                  alpha=alpha_i)\n",
        "\n",
        "def linearTransformation(transformMatrix):\n",
        "    orange = '#FF9A13'\n",
        "    blue = '#1190FF'\n",
        "    # Create original set of unit vectors\n",
        "    t = np.linspace(0, 2*np.pi, 100)\n",
        "    x = np.cos(t)\n",
        "    y = np.sin(t)\n",
        "\n",
        "    # Calculate eigenvectors and eigenvalues\n",
        "    eigVecs = np.linalg.eig(transformMatrix)[1]\n",
        "    eigVals = np.diag(np.linalg.eig(transformMatrix)[0])\n",
        "\n",
        "    # Create vectors of 0 to store new transformed values\n",
        "    newX = np.zeros(len(x))\n",
        "    newY = np.zeros(len(x))\n",
        "    for i in range(len(x)):\n",
        "        unitVector_i = np.array([x[i], y[i]])\n",
        "        # Apply the matrix to the vector\n",
        "        newXY = transformMatrix.dot(unitVector_i)\n",
        "        newX[i] = newXY[0]\n",
        "        newY[i] = newXY[1]\n",
        "\n",
        "    plotVectors([eigVecs[:,0], eigVecs[:,1]],\n",
        "                cols=[blue, blue])\n",
        "    plt.plot(x, y)\n",
        "\n",
        "    plotVectors([eigVals[0,0]*eigVecs[:,0], eigVals[1,1]*eigVecs[:,1]],\n",
        "                cols=[orange, orange])\n",
        "    plt.plot(newX, newY)\n",
        "    plt.xlim(-5, 5)\n",
        "    plt.ylim(-5, 5)\n",
        "    plt.axis('equal')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "P_D3tQR9LuZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([[1,-1], [-1, 4]])\n",
        "linearTransformation(A)"
      ],
      "metadata": {
        "id": "-nnjlHZPL353"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the unit circle in dark blue, the non scaled eigenvectors in light blue, the transformed unit circle in green and the scaled eigenvectors in yellow.\n",
        "\n",
        "It is worth noting that the eigenvectors are orthogonal here because the matrix is symmetric. Let's try with a non-symmetric matrix:"
      ],
      "metadata": {
        "id": "DwhzzUbCL81x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([[1,1], [-1, 4]])\n",
        "linearTransformation(A)"
      ],
      "metadata": {
        "id": "2PmKBA4SL5zP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, the eigenvectors are not orthogonal!"
      ],
      "metadata": {
        "id": "ukM6lBshMByk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eigenvalues of an Inverse\n",
        "-------------------------\n",
        "\n",
        "An invertible matrix cannot have an eigenvalue equal to zero. Furthermore, the eigenvalues of the inverse matrix are equal to the inverse of the eigenvalues of the original matrix:\n",
        "\n",
        "$$\n",
        "\\mathbf{A} {\\bf x} = \\lambda {\\bf x}\\implies \\\\ \\mathbf{A}^{-1} \\mathbf{A} {\\bf x} = \\lambda \\mathbf{A}^{-1} {\\bf x} \\implies \\\\ {\\bf x} = \\lambda \\mathbf{A}^{-1} {\\bf x}\\implies \\\\ \\mathbf{A}^{-1} {\\bf x} = \\frac{1}{\\lambda} {\\bf x}.\n",
        "$$\n",
        "\n",
        "\n",
        "Diagonalizability\n",
        "-----------------\n",
        "\n",
        "An $n \\times n$ matrix with $n$ linearly independent eigenvectors can be expressed as its eigenvalues and eigenvectors as:\n",
        "\n",
        "$$\n",
        "\\mathbf{A}\\mathbf{X} = \\begin{bmatrix} \\vert & & \\vert \\\\ \\lambda_1 {\\bf x}_1 & \\dots & \\lambda_n {\\bf x}_n \\\\ \\vert & & \\vert \\end{bmatrix} = \\begin{bmatrix} \\vert & & \\vert \\\\ {\\bf x}_1 & \\dots & {\\bf x}_n \\\\ \\vert & & \\vert \\end{bmatrix} \\begin{bmatrix}\\lambda_1 & & \\\\ & \\ddots & \\\\ & & \\lambda_n \\end{bmatrix} = \\mathbf{X}\\mathbf{D}\n",
        "$$\n",
        "\n",
        "The eigenvector matrix can be inverted to obtain the following **_similarity transformation_** of $\\mathbf{A}$:\n",
        "\n",
        "$$\n",
        "\\mathbf{AX} = \\mathbf{XD} \\iff \\mathbf{A} = \\mathbf{XDX}^{-1} \\iff \\mathbf{X}^{-1}\\mathbf{A}\\mathbf{X} = \\mathbf{D}\n",
        "$$\n",
        "\n",
        "Multiplying the matrix $\\mathbf{A}$ by $\\mathbf{X}^{-1}$ on the left and $\\mathbf{X}$ on the right transforms it into a diagonal matrix; it has been ‘‘diagonalized’’.\n",
        "\n",
        "#### Example: Matrix that is diagonalizable\n",
        "\n",
        "A $n \\times n$ matrix is diagonalizable if and only if it has $n$ linearly independent eigenvectors. For example:\n",
        "\n",
        "$$\n",
        "\\overbrace{\\begin{bmatrix} 1/6 & -1/3 & 1/6 \\\\ -1/2 & 0 & 1/2 \\\\ 1/3 & 1/3 & 1/3 \\end{bmatrix}}^{\\mathbf{X}^{-1}} \\overbrace{\\begin{bmatrix} 1 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix}}^{\\mathbf{A}} \\overbrace{\\begin{bmatrix} 1 & -1 & 1 \\\\ -2 & 0 & 1 \\\\ 1 & 1 & 1 \\end{bmatrix}}^{\\mathbf{X}} = \\overbrace{\\begin{bmatrix} -1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 2 \\end{bmatrix}}^{\\mathbf{D}}\n",
        "$$\n",
        "\n",
        "#### Example: Matrix that is not diagonalizable\n",
        "\n",
        "A matrix $\\mathbf{A}$ with linearly dependent eigenvectors is not diagonalizable. For example, while it is true that\n",
        "\n",
        "$$\n",
        "\\overbrace{\\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}}^{\\mathbf{A}} \\overbrace{\\begin{bmatrix} 1 & 1 \\\\ 0 & 0 \\end{bmatrix}}^{\\mathbf{X}} = \\overbrace{\\begin{bmatrix} 1 & 1 \\\\ 0 & 0 \\end{bmatrix}}^{\\mathbf{X}} \\overbrace{\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}}^{\\mathbf{D}},\n",
        "$$\n",
        "\n",
        "the matrix $\\mathbf{X}$ does not have an inverse, so we cannot diagonalize $\\mathbf{A}$ by applying an inverse. In fact, for any non-singular matrix $\\mathbf{P}$, the product $\\mathbf{P}^{-1}\\mathbf{AP}$ is not diagonal."
      ],
      "metadata": {
        "id": "Lb20pVvtYsg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy.linalg as la\n",
        "def diagonalize(A):\n",
        "    # A: nxn matrix\n",
        "    m, n = np.shape(A)\n",
        "    if (m != n):\n",
        "      return None\n",
        "\n",
        "    evals, evecs = la.eig(A) # eigenvectors as columns\n",
        "    if (la.matrix_rank(evecs) != n):\n",
        "      return None\n",
        "\n",
        "    D = np.diag(evals)\n",
        "    X = evecs\n",
        "    return (D, X)\n",
        "\n",
        "\n",
        "matrix = np.array([[1,1,0],\n",
        "                   [1,0,1],\n",
        "                   [0,1,1]])\n",
        "D, X = diagonalize(matrix)\n",
        "print(D)\n",
        "print(X)\n"
      ],
      "metadata": {
        "id": "XJ3wx9eUYvvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real Symmetric Matrix\n",
        "\n",
        "In the case of real symmetric matrices, the eigendecomposition can be expressed as\n",
        "\n",
        "$$ {A} = {Q}\\Lambda {Q}^\\text{T}$$\n",
        "\n",
        "where $Q$ is the matrix with eigenvectors as columns and $Λ$ is $\\text{diag}(\\lambda)$\n",
        "\n"
      ],
      "metadata": {
        "id": "7hnOpOD8_Xys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([[6, 2], [2, 3]])\n",
        "A"
      ],
      "metadata": {
        "id": "anco8Syt_UfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eigVals, eigVecs = np.linalg.eig(A)\n",
        "print(eigVals); print(eigVecs)"
      ],
      "metadata": {
        "id": "tRT7GTIaA650"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "D = np.diag(eigVals)\n",
        "print(D)"
      ],
      "metadata": {
        "id": "3zV2MR5oA8h5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eigVecs.dot(D).dot(eigVecs.T)"
      ],
      "metadata": {
        "id": "My4_WlPHBD6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show that Q'Q=I\n",
        "np.round( eigVecs.T@eigVecs ,10) # rounded for visibility (precision errors...)"
      ],
      "metadata": {
        "id": "Lf0G8NY4er_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# just some random matrix\n",
        "A = np.random.randint(-3,4,(3,3))\n",
        "\n",
        "# and make it symmetric\n",
        "A = A.T@A\n",
        "\n",
        "# its eigendecomposition\n",
        "L,Q = np.linalg.eig(A)\n",
        "\n",
        "# all pairwise dot products\n",
        "print( np.dot(Q[:,0],Q[:,1]) )\n",
        "print( np.dot(Q[:,0],Q[:,2]) )\n",
        "print( np.dot(Q[:,1],Q[:,2]) )"
      ],
      "metadata": {
        "id": "l_T-vwMrfGQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# real-valued matrix with complex-valued eigenvalues\n",
        "\n",
        "# a matrix\n",
        "A = np.array([[-3, -3, 0],\n",
        "              [ 3, -2, 3],\n",
        "              [ 0,  1, 2]])\n",
        "\n",
        "\n",
        "# btw, random matrices often have complex eigenvalues (though this is not guaranteed):\n",
        "#A = np.random.randint(-3,4,(3,3))\n",
        "\n",
        "# its eigendecomposition\n",
        "L,V = np.linalg.eig(A)\n",
        "L.reshape(-1,1) # print as column vector"
      ],
      "metadata": {
        "id": "x38WoIfRp6Cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a singular matrix\n",
        "A = np.array([[1,4,7],\n",
        "              [2,5,8],\n",
        "              [3,6,9]])\n",
        "\n",
        "# its eigendecomposition\n",
        "L,V = np.linalg.eig(A)\n",
        "\n",
        "\n",
        "# print its rank...\n",
        "print( f'Rank = {np.linalg.matrix_rank(A)}\\n' )\n",
        "\n",
        "# ... and its eigendecomposition\n",
        "print('Eigenvalues: ')\n",
        "print(L.round(2)), print(' ')\n",
        "\n",
        "print('Eigenvectors:')\n",
        "print(V.round(2))"
      ],
      "metadata": {
        "id": "1Lgb_vxnLxgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generalized eigendecomposition"
      ],
      "metadata": {
        "id": "MPj51LQbp6Qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 4\n",
        "\n",
        "# create symmetric matrices\n",
        "A = np.random.randn(n,n)\n",
        "A = A.T@A\n",
        "\n",
        "# impose a correlation between the two matrices (this improves numerical stability of the simultaneousl diagonalization)\n",
        "B = np.random.randn(n,n)\n",
        "B = B.T@B + A/10\n",
        "\n",
        "\n",
        "# using scipy\n",
        "from scipy.linalg import eigh\n",
        "evals,evecs = eigh(A,B)\n",
        "evals"
      ],
      "metadata": {
        "id": "omA9EZhWTHoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expressing an Arbitrary Vector as a Linear Combination of Eigenvectors\n",
        "----------------------------------------------------------------------\n",
        "\n",
        "If an $n\\times n$ matrix $\\mathbf{A}$ is diagonalizable, then we can write an arbitrary vector as a linear combination of the eigenvectors of $\\mathbf{A}$. Let ${\\bf u}_1,{\\bf u}_2,\\dots,{\\bf u}_n$ be $n$ linearly independent eigenvectors of $\\mathbf{A}$; then an arbitrary vector $\\mathbf{x}_0$ can be written:\n",
        "\n",
        "$$\n",
        "{\\bf x}_0 = \\alpha_1 {\\bf u}_1 + \\alpha_2 {\\bf u}_2 + \\dots + \\alpha_n {\\bf u}_n.\n",
        "$$\n",
        "\n",
        "If we apply the matrix $\\mathbf{A}$ to $\\mathbf{x}_0$:\n",
        "\n",
        "$$\n",
        "\\begin{align} \\mathbf{A}{\\bf x}_0 &= \\alpha_1 \\mathbf{A}{\\bf u}_1 + \\alpha_2\\mathbf{A}{\\bf u}_2 + \\dots + \\alpha_n \\mathbf{A}{\\bf u}_n, \\\\ &= \\alpha_1 \\lambda_1 {\\bf u}_1 + \\alpha_2\\lambda_2 {\\bf u}_2 + \\dots + \\alpha_n \\lambda_n {\\bf u}_n, \\\\ &= \\lambda_1 \\left(\\alpha_1 {\\bf u}_1 + \\alpha_2\\frac{\\lambda_2}{\\lambda_1}{\\bf u}_2 + \\dots + \\alpha_n \\frac{\\lambda_n}{\\lambda_1} {\\bf u}_n\\right). \\\\ \\end{align}\n",
        "$$\n",
        "\n",
        "If we repeatedly apply $\\mathbf{A}$ we have\n",
        "\n",
        "$$\n",
        "\\mathbf{A}^k{\\bf x}_0= \\lambda_1^k \\left(\\alpha_1 {\\bf u}_1 + \\alpha_2\\left(\\frac{\\lambda_2}{\\lambda_1}\\right)^k{\\bf u}_2 + \\dots + \\alpha_n \\left(\\frac{\\lambda_n}{\\lambda_1}\\right)^k {\\bf u}_n\\right).\n",
        "$$\n",
        "\n",
        "In the case where one eigenvalue has magnitude that is strictly greater than all the others, i.e.\n",
        "\n",
        "$\\vert\\lambda_1\\vert > \\vert\\lambda_2\\vert\\geq \\vert\\lambda_3\\vert \\geq \\dots \\geq\\vert\\lambda_n\\vert$,\n",
        "\n",
        "this implies\n",
        "\n",
        "$$\n",
        "\\lim_{k\\to\\infty}\\frac{\\mathbf{A}^k {\\bf x}_0}{\\lambda_1^{k}} = \\alpha_1 {\\bf u}_1.\n",
        "$$\n",
        "\n",
        "This observation motivates the algorithm known as **_power iteration_**, which is the topic of the next section.\n",
        "\n",
        "Power Iteration algorithm\n",
        "-------------------------\n",
        "\n",
        "For a matrix ${\\bf A}$, power iteration will find a scalar multiple of an eigenvector ${\\bf u}_1$, corresponding to the dominant eigenvalue (largest in magnitude) $\\lambda_1$, provided that $\\vert\\lambda_1\\vert$ is strictly greater than the magnitude of the other eigenvalues, i.e., $\\vert\\lambda_1\\vert > \\vert\\lambda_2\\vert \\ge \\dots \\ge \\vert\\lambda_n\\vert$.\n",
        "\n",
        "Suppose\n",
        "\n",
        "$\\mathbf{x}_0 = \\alpha_1 \\mathbf{u}_1 + \\alpha_2\\mathbf{u}_2 + \\dots \\alpha_n\\mathbf{u}_n,\\text{ with }\\alpha_1 \\neq 0$.\n",
        "\n",
        "From the previous section, the iterative sequence\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_k = \\mathbf{A}\\mathbf{x}_{k-1}\\text{ for }k=1,2,3,\\dots\n",
        "$$\n",
        "\n",
        "satisfies\n",
        "\n",
        "$\\mathbf{x}_k = \\mathbf{A}^k\\mathbf{x}_0 \\implies \\lim_{k\\to\\infty}\\frac{\\mathbf{x}_k}{\\lambda_1^k} = \\alpha_1\\mathbf{u}_1$.\n",
        "\n",
        "Thus, for large $k$, $\\mathbf{x}_k\\approx \\lambda_1^k \\alpha_1\\mathbf{u}_1$. Unfortunately, this mean that $\\|\\mathbf{x}_k\\| \\approx |\\lambda_1|^k\\cdot\\|\\alpha_1\\mathbf{u}_1\\|,$ which will be very large if $|\\lambda_1| > 1$, or very small if $ |\\lambda_1| < 1 $. For this reason, we use **_normalized_** power iteration.\n",
        "\n",
        "Normalized power iteration, is given by the following. Let $\\mathbf{x}_0$ be a vector with unit norm: $\\|\\mathbf{x}_0\\| = 1$ (any norm is fine), with $\\mathbf{x}_0 = \\alpha_1 \\mathbf{u}_1 + \\alpha_2\\mathbf{u}_2 + \\dots \\alpha_n\\mathbf{u}_n,\\text{ and }\\alpha_1 \\neq 0$.\n",
        "\n",
        "**_Normalized power iteration_** is defined by the following iterative sequence for $k=1,2,3,\\dots$:\n",
        "\n",
        "$$\n",
        "\\begin{align} &\\mathbf{y}_k = \\mathbf{A}\\mathbf{x}_{k-1} \\\\ &\\mathbf{x}_k = \\frac{\\mathbf{y}_k}{\\|\\mathbf{y}_k\\|} \\end{align}\n",
        "$$\n",
        "\n",
        "where the norm $\\|\\cdot\\|$ is identical to the norm used when we assumed $\\|\\mathbf{x}_0\\| = 1$.\n",
        "\n",
        "It can be shown that this sequence satisfies\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_k = \\frac{\\mathbf{A}^k\\mathbf{x}_0}{\\|\\mathbf{A}^k\\mathbf{x}_0\\|}.\n",
        "$$\n",
        "\n",
        "This means that for large values of $k$, we have\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_k \\approx \\left(\\frac{\\lambda_1}{|\\lambda_1|}\\right)^k\\cdot\\frac{\\alpha_1\\mathbf{u}_1}{\\|\\alpha_1\\mathbf{u}_1\\|}.\n",
        "$$\n",
        "\n",
        "The largest eigenvalue could be positive, negative, or a complex number. In each case we will have:\n",
        "\n",
        "$$\n",
        "\\begin{align} \\lambda_1 > 0 \\implies &\\mathbf{x}_k \\approx \\frac{\\alpha_1\\mathbf{u}_1}{\\|\\alpha_1\\mathbf{u}_1\\|}\\hspace{22.5mm} \\mathbf{x}_k\\text{ converges} \\\\ \\lambda_1 < 0 \\implies &\\mathbf{x}_k \\approx (-1)^k \\frac{\\alpha_1\\mathbf{u}_1}{\\|\\alpha_1\\mathbf{u}_1\\|}\\hspace{11.5mm} \\text{in the limit, }\\mathbf{x}_k\\text{ alternates between }\\pm\\frac{\\alpha_1\\mathbf{u}_1}{\\|\\alpha_1\\mathbf{u}_1\\|}\\\\ \\lambda_1 = re^{i\\theta} \\implies & \\mathbf{x}_k \\approx e^{ik\\theta} \\frac{\\alpha_1\\mathbf{u}_1}{\\|\\alpha_1\\mathbf{u}_1\\|} \\hspace{16mm} \\text{in the limit, }\\mathbf{x}_k \\text{ is a scalar multiple of } \\mathbf{u}_1 \\text{ with coefficient that rotates around the unit circle}. \\end{align}\n",
        "$$\n",
        "\n",
        "Strictly speaking, normalized power iteration only converges to a single vector if $\\lambda_1 > 0$, but $\\mathbf{x}_k$ will be close to a scalar multiple of the eigenvector $\\mathbf{u}_1$ for large values of $k$, regardless of whether the dominant eigenvalue is positive, negative, or complex. So normalized power iteration will work for any value of $\\lambda_1$, as long as it is strictly bigger in magnitude than the other eigenvalues.\n",
        "\n",
        "Power Iteration code\n",
        "--------------------\n",
        "\n",
        "The following code snippet performs power iteration:\n",
        "\n",
        "    import numpy as np\n",
        "    def power_iter(A, x_0, p):\n",
        "      # A: nxn matrix, x_0: initial guess, p: type of norm\n",
        "      x_0 = x_0/np.linalg.norm(x_0,p)\n",
        "      x_k = x_0\n",
        "      for i in range(max_iterations):\n",
        "        y_k = A @ x_k\n",
        "        x_k = y_k/np.linalg.norm(y_k,p)\n",
        "      return x_k\n",
        "    \n",
        "\n",
        "#### Example: Two Steps of Power Iteration\n",
        "\n",
        "We’ll use normalized power iteration (with the infinity norm) to approximate an eigenvector of the following matrix: $\\mathbf{A} = \\begin{bmatrix} 1 & -2 \\\\ -1 & 1 \\end{bmatrix},$ and the following initial guess: $\\mathbf{x}_0 = \\begin{bmatrix} -1 \\\\ 0 \\end{bmatrix}$\n",
        "\n",
        "**First Iteration**:\n",
        "\n",
        "$$\n",
        " \\begin{align} &\\mathbf{y}_1 = \\mathbf{A}\\mathbf{x}_0 = \\begin{bmatrix} 1 & -2 \\\\ -1 & 1 \\end{bmatrix} \\begin{bmatrix} -1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix},\\\\[15pt] &\\mathbf{x}_1 = \\frac{\\mathbf{y}_1}{\\|\\mathbf{y}_1\\|_{\\infty}} = \\mathbf{y}_1 = \\begin{bmatrix} -1 \\\\ 1\\end{bmatrix}. \\end{align}\n",
        "$$\n",
        "\n",
        "**Second Iteration**:\n",
        "\n",
        "$$\n",
        "\\begin{align} &\\mathbf{y}_2 = \\mathbf{A}\\mathbf{x}_1 = \\begin{bmatrix} 1 & -2 \\\\ -1 & 1 \\end{bmatrix} \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} -3 \\\\ 2 \\end{bmatrix},\\\\[15pt] &\\mathbf{x}_2 = \\frac{\\mathbf{y}_2}{\\|\\mathbf{y}_2\\|_{\\infty}} = \\frac{1}{3}\\mathbf{y}_2= \\begin{bmatrix} -1 \\\\ \\frac{2}{3}\\end{bmatrix} = \\begin{bmatrix} -1 \\\\ 0.6666\\dots\\end{bmatrix}. \\end{align}\n",
        "$$\n",
        "\n",
        "Even after only two iterations, we are getting close to a corresponding eigenvector:\n",
        "\n",
        "$$\n",
        "\\mathbf{u}_1 = \\begin{bmatrix} -1 \\\\ \\frac{1}{\\sqrt{2}} \\end{bmatrix} \\approx \\begin{bmatrix} -1 \\\\ 0.7071 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "with relative error about 4 percent when measured in the infinity norm.\n",
        "\n",
        "Computing Eigenvalues from Eigenvectors\n",
        "---------------------------------------\n",
        "\n",
        "Power iteration allows us to find an approximate eigenvector corresponding to the largest eigenvalue in magnitude. How can we compute the actual eigenvalue from this? If $\\lambda \\text{ is an eigenvalue of }\\mathbf{A} \\text{, with corresponding eigenvector } \\mathbf{u}$, then we can compute the value of $\\lambda$ using the **_Rayleigh Quotient_**:\n",
        "\n",
        "$$\n",
        "\\lambda = \\frac{\\mathbf{u}^T\\mathbf{A}\\mathbf{u}}{\\mathbf{u}^T\\mathbf{u}}.\n",
        "$$\n",
        "\n",
        "Thus, one can compute an approximate eigenvalue using the approximate eigenvector found during power iteration.\n",
        "\n",
        "Power Iteration and Floating-Point Arithmetic\n",
        "---------------------------------------------\n",
        "\n",
        "Recall that we made the assumption that the initial guess satisfies\n",
        "\n",
        "$\\mathbf{x}_0 = \\alpha_1 \\mathbf{u}_1 + \\alpha_2\\mathbf{u}_2 + \\dots \\alpha_n\\mathbf{u}_n,\\text{ with }\\alpha_1 \\neq 0$.\n",
        "\n",
        "What happens if we choose an initial guess where $\\alpha_1 = 0$? If we further assume that $\\vert\\lambda_2\\vert > \\vert\\lambda_3\\vert\\geq \\vert\\lambda_4\\vert \\geq \\dots \\geq\\vert\\lambda_n\\vert$, then in theory\n",
        "\n",
        "$$\n",
        "\\mathbf{A}^k\\boldsymbol{x}_0= \\lambda_2^k \\left(\\alpha_2 {\\bf u}_2 + \\alpha_3\\left(\\frac{\\lambda_3}{\\lambda_2}\\right)^k{\\bf u}_3 + \\dots + \\alpha_n \\left(\\frac{\\lambda_n}{\\lambda_2}\\right)^k {\\bf u}_n\\right),\n",
        "$$\n",
        "\n",
        "and we would expect that\n",
        "\n",
        "$$\n",
        "\\lim_{k\\to\\infty}\\frac{\\mathbf{A}^k \\boldsymbol{x}_0}{\\lambda_2^{k}} = \\alpha_2 {\\bf u}_2.\n",
        "$$\n",
        "\n",
        "In practice, this does not happen. For one thing, choosing an initial guess such that $\\alpha_1 = 0$ is extremely unlikely if we have no prior knowledge about the eigenvector $\\mathbf{u}_1$. Since power iteration is performed numerically, using finite precision arithmetic, we will encounter the presence of rounding error in every iteration. This means that at every iteration $k\\text{, including }k = 0$, we will instead have\n",
        "\n",
        "$$\n",
        "\\mathbf{A}^k\\hat{\\boldsymbol{x}}_0= \\lambda_1^k \\left(\\hat{\\alpha}_1 \\boldsymbol{u}_1 + \\hat{\\alpha}_2\\left(\\frac{\\lambda_2}{\\lambda_1}\\right)^k\\boldsymbol{u}_2 + \\dots + \\hat{\\alpha}_n \\left(\\frac{\\lambda_n}{\\lambda_1}\\right)^k \\boldsymbol{u}_n\\right),\n",
        "$$\n",
        "\n",
        "where the $\\hat{\\alpha}_k$ are the approximate expansion coefficients of the rounded result. Even if $\\alpha_1 = 0$, the finite precision representation $\\hat{\\mathbf{x}}_0$, will very likely have expansion coefficient $\\hat{\\alpha}_1 \\neq 0$. Even in the case where rounding the initial guess does not introduce a non-zero $\\hat{\\alpha}_1$, rounding after applying the matrix $\\mathbf{A}$ will almost certainly introduce a non-zero component in the dominant eigenvector after enough iterations. The probability of coming up with a starting guess $\\mathbf{x}_0$ such that $\\hat{\\alpha}_1 = 0$ for all iterations is very, very low, if not impossible.\n",
        "\n",
        "Power Iteration without a Dominant Eigenvalue\n",
        "---------------------------------------------\n",
        "\n",
        "Above, we assumed that one eigenvalue had magnitude strictly larger than all the others: $\\vert\\lambda_1\\vert > \\vert\\lambda_2\\vert\\geq \\vert\\lambda_3\\vert \\geq \\dots \\geq\\vert\\lambda_n\\vert$. What happens if $\\vert\\lambda_1\\vert = \\vert\\lambda_2\\vert$?\n",
        "\n",
        "If $\\lambda_1 = \\lambda_2 = \\lambda \\in \\mathbb{R}$, then:\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_k = \\mathbf{A}^k\\mathbf{x}_0 \\approx \\alpha_1\\lambda^k\\mathbf{u}_1 + \\alpha_2\\lambda^k\\mathbf{u}_2 = \\lambda^k\\left(\\alpha_1\\mathbf{u}_1 + \\alpha_2\\mathbf{u}_2\\right),\n",
        "$$\n",
        "\n",
        "hence\n",
        "\n",
        "$\\lim_{k\\to\\infty}\\lambda^{-k}\\mathbf{A}^k\\mathbf{x}_0 = \\alpha_1\\mathbf{u}_1 + \\alpha_2\\mathbf{u}_2$.\n",
        "\n",
        "The quantity $\\alpha_1\\mathbf{u}_1 + \\alpha_2\\mathbf{u}_2$ is still an eigenvector corresponding to $\\lambda$, so power iteration will still approach a dominant eigenvector.\n",
        "\n",
        "If the dominant eigenvalues have opposite sign, i.e., $\\lambda_1 = -\\lambda_2 = \\lambda \\in \\mathbb{R}$, then\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_k = \\mathbf{A}^k\\mathbf{x}_0 \\approx \\alpha_1\\lambda^k\\mathbf{u}_1 + \\alpha_2(-\\lambda)^k\\mathbf{u}_2 = \\lambda^k\\left(\\alpha_1\\mathbf{u}_1 + (-1)^k\\alpha_2\\mathbf{u}_2\\right).\n",
        "$$\n",
        "\n",
        "For large $k$, we will have $\\lambda^{-k}\\mathbf{A}\\mathbf{x}_0 \\approx \\alpha_1\\mathbf{u}_1 + (-1)^k\\alpha_2\\mathbf{u}_2$, which although is a linear combination of two eigenvectors, is **_not_** itself an eigenvector of $\\mathbf{A}$.\n",
        "\n",
        "Finally, if the two dominant eigenvalues are a complex-conjugate pair $\\lambda_1 = re^{i\\theta},\\ \\lambda_2 = re^{-i\\theta}$, then $\\mathbf{x}_k = \\mathbf{A}^k\\mathbf{x}_0 \\approx \\alpha_1\\lambda^k\\mathbf{u}_1 + \\alpha_2(\\overline{\\lambda})^k\\mathbf{u}_2 = \\lambda^k\\left(\\alpha_1\\mathbf{u}_1 + \\left(\\frac{\\overline{\\lambda}}{\\lambda}\\right)^k\\alpha_2\\mathbf{u}_2\\right) = \\lambda^k(\\alpha_1\\mathbf{u}_1 + \\alpha_2 e^{-i2k\\theta}\\mathbf{u}_2).$\n",
        "\n",
        "For large $k$, $\\lambda^{-k}\\mathbf{A}\\mathbf{x}_0$ approximate a linear combination of two eigenvectors, but this linear combination will not itself be an eigenvector.\n",
        "\n",
        "Inverse Iteration\n",
        "-----------------\n",
        "\n",
        "To obtain an eigenvector corresponding to the **_smallest_** eigenvalue $\\lambda_n$ of a non-singular matrix, we can apply power iteration to $\\mathbf{A}^{-1}$. The following recurrence relationship describes inverse iteration algorithm: $\\boldsymbol{x}_{k+1} = \\frac{\\mathbf{A}^{-1} \\boldsymbol{x}_k}{\\|\\mathbf{A}^{-1} \\boldsymbol{x}_k\\|},$\n",
        "\n",
        "\n",
        "Convergence properties\n",
        "----------------------\n",
        "\n",
        "The convergence rate for power iteration is **_linear_** and the recurrence relationship for the error between the current iterate and a dominant eigenvector is given by: $\\mathbf{e}_{k+1} \\approx \\frac{|\\lambda_2|}{|\\lambda_1|}\\mathbf{e}_k$ The convergence rate for (shifted) inverse iteration is also linear, but now depends on the two closest eigenvalues to the shift $\\sigma$. (Standard inverse iteration corresponds to a shift $\\sigma = 0$. The recurrence relationship for the errors is given by: $\\mathbf{e}_{k+1} \\approx \\frac{|\\lambda_\\text{closest} - \\sigma|}{|\\lambda_\\text{second-closest} - \\sigma|}\\mathbf{e}_k$\n",
        "\n"
      ],
      "metadata": {
        "id": "dZeN9VOZDaqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ray_iter(A, v=None, lam=None, maxiter=1000, tol=1.0e-12):\n",
        "    m = A.shape[0]\n",
        "    if v is None:\n",
        "        v = 2 * np.random.random(m) - 1\n",
        "    v /= np.linalg.norm(v)\n",
        "    if lam is None:\n",
        "        lam = np.dot(v, A @ v)\n",
        "    print(0, lam)\n",
        "    for k in range(maxiter):\n",
        "        B = A - lam * np.eye(A.shape[0])\n",
        "        w = np.linalg.solve(B, v)\n",
        "        v = w /np.linalg.norm(w)\n",
        "        lam = np.dot(v, A @ v)\n",
        "        print(k+1, lam)\n",
        "        if np.linalg.norm(A@v - lam*v) < tol:\n",
        "            return v, lam\n",
        "    print('Did not converge !!!')\n",
        "    return v, lam\n",
        "\n",
        "def power_method(A, v, maxiter=1000, tol=1.0e-12):\n",
        "    v /= np.linalg.norm(v)\n",
        "    w = A @ v\n",
        "    lam = np.dot(v, w)\n",
        "    print(0, lam)\n",
        "    for k in range(maxiter):\n",
        "        v = w /np.linalg.norm(w)\n",
        "        w = A @ v\n",
        "        lam = np.dot(v, w)\n",
        "        print(k+1, lam)\n",
        "        if np.linalg.norm(A@v - lam*v) < tol:\n",
        "            return v, lam\n",
        "    print('Did not converge !!!')\n",
        "    return v, lam\n",
        "\n",
        "A = np.array([[2, 1, 1],\n",
        "              [1, 3, 1],\n",
        "              [1, 1, 4]])\n",
        "n = A.shape[0]\n",
        "v = np.ones(n)\n",
        "v, lam = power_method(A, v)\n",
        "# The eigenvalues converge quickly due to quadratic convergence, but eigenvectors converge slowly due to linear convergence.\n",
        "# The eigenvector closest to v has eigenvalue 5.214319743377534...\n",
        "\n",
        "print(\"\\n\",v)\n",
        "print(\"\\n\")\n",
        "\n",
        "v = np.array([1., 1., 1.])\n",
        "v, lam = ray_iter(A, v=v)\n",
        "print(\"\\n\",v)\n"
      ],
      "metadata": {
        "id": "tyqum5sooGIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inv_iter(A, mu, maxiter=1000, tol=1.0e-12):\n",
        "    v = 2 * np.random.rand(A.shape[0]) - 1\n",
        "    v /= np.linalg.norm(v)\n",
        "    lam = np.dot(v, A @ v)\n",
        "    print(0, lam)\n",
        "    for k in range(maxiter):\n",
        "        B = A - mu * np.eye(A.shape[0])\n",
        "        w = np.linalg.solve(B, v)\n",
        "        v = w /np.linalg.norm(w)\n",
        "        lam = np.dot(v, A @ v)\n",
        "        print(k+1, lam)\n",
        "        if np.linalg.norm(A@v - lam*v) < tol:\n",
        "            return v, lam\n",
        "    print('Did not converge !!!')\n",
        "    return v, lam\n",
        "\n",
        "A = np.array([[2, 1, 1],\n",
        "              [1, 3, 1],\n",
        "              [1, 1, 4]])\n",
        "mu = 1.0\n",
        "v, lam = inv_iter(A, mu)\n",
        "print(lam)\n",
        "print(\"\\n\",v)\n",
        "print(\"\\n\")\n",
        "\n",
        "v = 2 * np.random.rand(3) - 1\n",
        "v, lam = ray_iter(A, v=v)\n",
        "print(\"\\n\",v)\n",
        "print(\"\\n\")\n",
        "\n",
        "v, lam = ray_iter(A, lam=3.0)\n",
        "print(\"\\n\",v)\n",
        "print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "DZ3lcPvKoqw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Orthogonal Matrices\n",
        "-------------------\n",
        "\n",
        "Square matrices are called _orthogonal_ if and only if the columns are mutually orthogonal to one another and have a norm of $1$ (such a set of vectors are formally known as a _orthonormal_ set), i.e.: $\\boldsymbol{c}_i^T \\boldsymbol{c}_j = 0 \\quad \\forall \\ i \\neq j, \\quad \\|\\boldsymbol{c}_i\\| = 1 \\quad \\forall \\ i \\iff \\mathbf{A} \\in \\mathcal{O}(n),$ or $\\langle\\boldsymbol{c}_i,\\boldsymbol{c}_j \\rangle = \\begin{cases} 0 \\quad \\mathrm{if} \\ i \\neq j, \\\\ 1 \\quad \\mathrm{if} \\ i = j \\end{cases} \\iff \\mathbf{A} \\in \\mathcal{O}(n),$ where $\\mathcal{O}(n)$ is the set of all $n \\times n$ orthogonal matrices called the orthogonal group, $\\boldsymbol{c}_i$, $i=1, \\dots, n$, are the columns of $\\mathbf{A}$, and $\\langle \\cdot, \\cdot \\rangle$ is the inner product operator. Orthogonal matrices have many desirable properties: $\\mathbf{A}^T \\in \\mathcal{O}(n) \\\\ \\mathbf{A}^T \\mathbf{A} = \\mathbf{A} \\mathbf{A}^T = \\mathbf{I} \\implies \\mathbf{A}^{-1} = \\mathbf{A}^T \\\\ \\det{\\mathbf{A}} = \\pm 1 \\\\ \\kappa_2(\\mathbf{A}) = 1$\n",
        "\n",
        "Gram-Schmidt\n",
        "------------\n",
        "\n",
        "The algorithm to construct an orthogonal basis from a set of linearly independent vectors is called the Gram-Schmidt process. For a basis set $\\{x_1, x_2, \\dots x_n\\}$, we can form a orthogonal set $\\{v_1, v_2, \\dots v_n\\}$ given by the following transformation:\n",
        "$$  \\begin{align} \\boldsymbol{v}_1 &= \\boldsymbol{x}_1, \\\\ \\boldsymbol{v}_2 &= \\boldsymbol{x}_2 - \\frac{\\langle\\boldsymbol{v}_1,\\boldsymbol{x}_2\\rangle}{\\|\\boldsymbol{v}_1\\|^2}\\boldsymbol{v}_1\\\\ \\boldsymbol{v}_3 &= \\boldsymbol{x}_3 - \\frac{\\langle\\boldsymbol{v}_1,\\boldsymbol{x}_3\\rangle}{\\|\\boldsymbol{v}_1\\|^2}\\boldsymbol{v}_1 - \\frac{\\langle\\boldsymbol{v}_2,\\boldsymbol{x}_3\\rangle}{\\|\\boldsymbol{v}_2\\|^2}\\boldsymbol{v}_2\\\\ \\vdots &= \\vdots\\\\ \\boldsymbol{v}_n &= \\boldsymbol{x}_n - \\sum_{i=1}^{n-1}\\frac{\\langle\\boldsymbol{v}_i,\\boldsymbol{x}_n\\rangle}{\\|\\boldsymbol{v}_i\\|^2}\\boldsymbol{v}_i, \\end{align}$$\n",
        " where $\\langle \\cdot, \\cdot \\rangle$ is the inner product operator. Each of the vectors in the orthogonal set can be normalized independently to obtain a orthonormal basis.\n"
      ],
      "metadata": {
        "id": "XlF8WdMdriM4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAbpeoC3p2kQ"
      },
      "source": [
        "### Pseudocode for Gram-Schmidt\n",
        "\n",
        "Given a matrix $\\mathbf{A} = \\begin{bmatrix} \\vec{x}_1 \\vert \\vec{x}_2 \\vert \\ldots \\vert \\vec{x}_n \\end{bmatrix}$\n",
        "\n",
        "| Steps |     |\n",
        "| --:   | :-- |\n",
        "| 1.    | For  $j=1, 2, \\ldots, n$ do Step 2-3\n",
        "| 2.    | $\\phantom{--}$ Set  $\\vec{v}_j=\\vec{x}_j$\n",
        "| 3.    | For  $j=1, 2, \\ldots, n$ do Step 4-5\n",
        "| 4.    | $\\phantom{--}$ Set  $\\vec{q}_j=\\vec{v}_j / \\vert \\vec{v}_j \\vert$\n",
        "| 5.    | $\\phantom{--}$ For  $k=j+1, 2, \\ldots, n$ do Step 6\n",
        "| 6.    | $$\\phantom{----} \\vec{v}_k=\\vec{v}_k-\\left(\\vec{q}^\\intercal_k \\vec{v}_j\\right)\\vec{q}_k$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XH3VFyRYp2kQ"
      },
      "source": [
        "### Python/NumPy implementation of Gram-Schmidt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "XDKeZJgXp2kQ"
      },
      "outputs": [],
      "source": [
        "def gram_schmidt(A):\n",
        "    m, n = A.shape\n",
        "    Q, R = np.empty((m,n)), np.zeros((n,n))\n",
        "    for j in range(n):\n",
        "        v = A[:,j]\n",
        "        for i in range(j):\n",
        "            R[i,j] = Q[:,i].dot(A[:,j])\n",
        "            v = v - R[i,j] * Q[:,i]\n",
        "        R[j,j] = np.linalg.norm(v, ord=2)\n",
        "        Q[:,j] = v / R[j,j]\n",
        "    return Q, R"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m, n = 10, 5\n",
        "A = 2 * np.random.rand(m,n) - 1\n",
        "Q, R = gram_schmidt(A)\n",
        "np.abs(A - Q @ R).max()"
      ],
      "metadata": {
        "id": "x1GqdLfhYcHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.abs(Q.T @ Q - np.eye(n)).max()\n",
        "print(Q.T @ Q)"
      ],
      "metadata": {
        "id": "gUkmaK2OZkUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NM_oiGnp2kQ"
      },
      "source": [
        "## QR Factorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Stbl1SXDp2kR"
      },
      "source": [
        "Let $\\mathbf{A}$ be any real matrix with linearly independent columns. Then there exists an orthogonal matrix $\\mathbf{A}$ triangular matrix $\\mathbf{A}$ having non-negative entries on the main diagonal such that: $\\mathbf{A=QR}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgVH2q0jp2kR"
      },
      "source": [
        "Recall that $\\mathbf{Q}^{-1}$ = $\\mathbf{Q}^{T}$, we have\n",
        "$$\n",
        "\\mathbf{R} = \\mathbf{Q}^{T}\\mathbf{A}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y39ch0Oip2kS"
      },
      "source": [
        "## QR Algorithm for computing eigenvalues"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er9ctD1Fp2kS"
      },
      "source": [
        "This algorithm is so simple: we start with a matrix $\\mathbf{A}^{(0)}$ and perform $\\mathbf{QR}$ factorization,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7XHvej9p2kS"
      },
      "source": [
        "$$\\mathbf{A}^{(0)} = \\mathbf{Q}^{(0)}\\mathbf{R}^{(0)}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0UXrOvGp2kS"
      },
      "source": [
        "Then we reverse the factors to form the matrix $\\mathbf{A}^{(1)}$,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "remove_cell"
        ],
        "id": "3ukiws89p2kT"
      },
      "source": [
        "$$\\mathbf{A}^{(1)} = \\mathbf{R}^{(0)}\\mathbf{Q}^{(0)}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znvRpSFjp2kT"
      },
      "source": [
        "We can do this because $\\mathbf{A}^{(1)}$ is similar to $\\mathbf{A}^{(0)}$, we can verify\n",
        "\n",
        "$$\\mathbf{Q}^{(0)^{-1}}\\mathbf{A}^{(0)}\\mathbf{Q}^{(0)} = \\mathbf{Q}^{(0)^{-1}}(\\mathbf{Q}^{(0)}\\mathbf{R}^{(0)})\\mathbf{Q}^{(0)} = \\mathbf{A}^{(1)}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nZhnA2Hp2kT"
      },
      "source": [
        "And we continue this process without changing the eigenvalues. The above process is called the *unshifted QR algorithm*, and almost always $\\mathbf{A}^{(k)}$ approaches a triangular form, its diagonal entries approach its eigenvalues, which are also the eigenalues of $\\mathbf{A}^{(0)}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpmuyrlwp2kU"
      },
      "source": [
        "### Python/NumPy implementation of QR eigenvalue algorithm\n",
        "\n",
        "The built in `numpy.linalg` packaged functions to compute eigenvalues use the $\\mathbf{QR}$ algorithm; the implementation below simply uses the matrix operations for clarity in the method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "lines_to_next_cell": 2,
        "id": "j39vNhzrp2kU"
      },
      "outputs": [],
      "source": [
        "def qr_classical(A, maxiter=1000, tol=1e-12):\n",
        "    for k in range(maxiter):\n",
        "        Q, R = np.linalg.qr(A)\n",
        "        A = R @ Q\n",
        "        sub_diag = np.diag(A,-1)\n",
        "        if np.abs(sub_diag).max() < tol:\n",
        "            print('Converged in iterations = ', k+1)\n",
        "            return A\n",
        "    print('Did not converge in maxiter =', maxiter)\n",
        "    return A"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = 5\n",
        "B = np.random.random((m,m))\n",
        "A = 0.5 * (B + B.T)\n",
        "evals,evecs = np.linalg.eig(A)\n",
        "print('Eigenvalues = ', evals)\n"
      ],
      "metadata": {
        "id": "OMlTS_DSvELN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "D = qr_classical(A.copy())\n",
        "print(np.array_str(D,suppress_small=True))"
      ],
      "metadata": {
        "id": "Blb9rGcQvQ5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 4"
      ],
      "metadata": {
        "id": "j3O8j31bsYyb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HW4-1:** One interesting property of random matrices is that their complex-valued eigenvalues\n",
        "are distributed in a circle with a radius proportional to the size of the matrix. Compute 93 random 32 by 32 matrices, extract their eigenvalues, divide by the square root of the matrix size (32), and plot the eigenvalues on the complex plane. What do you observe?"
      ],
      "metadata": {
        "id": "4sMhXTLeiwNW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HW4-2**: The orthonormality of columns of $Q$ is not accurate to machine precision using the Gram-Schmidt algorithm above. Test this on a matrix whose columns are almost parallel and compare with NumPy Gram-Schmidt algorithm:\n",
        "\n",
        "```python\n",
        "  A = np.array([[0.70000, 0.70711],\n",
        "                [0.70001, 0.70711]])\n",
        "  Q, R = np.linalg.qr(A)\n",
        "  print('NumPy  QR = ',np.abs(Q.T @ Q - np.eye(2)).max())\n",
        "  Q, R = gram_schmidt(A)\n",
        "  print('Classical GS QR = ',np.abs(Q.T @ Q - np.eye(2)).max())\n",
        "```\n",
        "\n",
        "First, construct a square matrix $A$ with random singular vectors and widely varying singular values spaced by factors of 2 between $2^{-1}$ and $2^{-80}$:\n",
        "$$\n",
        "    A = U S V,\n",
        "$$\n",
        "where $S=\\textrm{ diag}[2^{-1}, 2^{-2}, \\ldots, 2^{-80}]$ and $U,V$ are random unitary matrices generated using NumPy QR factorization. Then use Gram-Schmidt function above to compute the QR factorization of $A$. Then plot the diagonal elements of $R$ produced by the above algorithm and produced by the NumPy QR factorization. Describe your observation."
      ],
      "metadata": {
        "id": "jVPuZ2g0U1we"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "MHzzHVn6y8pL"
      },
      "source": [
        "**HW4-3 (Practical QR with shifts)**: The above classical QR algorithm is really really slow. In this homework, use the following idea to speed up the convergence:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "JP1byBCry8pM"
      },
      "source": [
        "Instead of factoring $A_k$ as $Q_k R_k$,\n",
        "\n",
        "1. Get the QR factorization $$A_k - s_k I = Q_k R_k$$\n",
        "2. Set $$A_{k+1} = R_k Q_k + s_k I$$\n",
        "\n",
        "For simplicity, choose the bottom-right element as a shift $s_k$ to approximate an eigenvalue of $A$, i.e. $s_k = A_k(n-1,n-1)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqT-k2Swy8pM"
      },
      "source": [
        "#### Assignment: Add shifts to the QR algorithm and compare the number of iterations with the classical QR algorithm above."
      ]
    }
  ]
}